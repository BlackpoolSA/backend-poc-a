"""
Tool Vector Search para búsqueda de vectores en Oracle Database.
"""
import logging
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# Primero importar las dependencias para resolver referencias circulares
from langchain_core.caches import BaseCache
from langchain_core.callbacks.manager import Callbacks
# Luego importar ChatOCIGenAI
from langchain_community.chat_models import ChatOCIGenAI

# Reconstruir el modelo Pydantic para evitar el error de clase no definida
ChatOCIGenAI.model_rebuild(_types_namespace={"BaseCache": BaseCache, "Callbacks": Callbacks})

from database.rag_docs import RAGDocsDB
from core.config import settings

logger = logging.getLogger(__name__)

class OCIRAGTool:
    """
    Servicio para ejecutar búsqueda de vectores en Oracle Database.
    """
    
    def __init__(self):
        self.rag_docs_db = RAGDocsDB()
        
        # Initialize the OCI Generative AI Chat Model
        self.llm = ChatOCIGenAI(
            model_id         = settings.CON_GEN_AI_CHAT_MODEL_ID,
            service_endpoint = settings.CON_GEN_AI_SERVICE_ENDPOINT,
            compartment_id   = settings.CON_COMPARTMENT_ID,
            provider         = settings.CON_GEN_AI_CHAT_MODEL_PROVIDER,
            is_stream        = False,
            auth_type        = "API_KEY",
            auth_profile     = settings.OCI_PROFILE
        )

    def oci_vector_search(self, input: str, files_ids: list) -> str:
        """
        Performs a vector search using an Oracle database and OCI embeddings.
        
        Args:
            input (str): The input query to search for relevant documents.
            
        Returns:
            str: The answer generated by the retrieval-augmented generation (RAG) chain.
        """
        
        # Create a vector store from the Oracle connection and embeddings
        vector_store = self.rag_docs_db.get_vector_store()

        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 10,         # Número de chunks relevantes que se devuelven
                "fetch_k": 20,   # Número de candidatos iniciales desde los cuales aplicar MMR
                "filter": {"file_id": files_ids} # Limita la búsqueda al archivo específico
            }
        )

        # Define a prompt template for retrieval-augmented generation (RAG) using the retrieved context fragments
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", (
                    """
                    You are an assistant for question-answering tasks.
                    Please use only the following retrieved context fragments to answer the question.
                    If you don't know the answer, say you don't know.
                    Always use all available data.

                    {context}
                    """
                )),
                ("human", "{input}")
            ]
        )

        # Create a chain that uses the LLM to combine the retrieved documents into a final answer
        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)
        
        # Build the full retrieval chain that first retrieves documents and then uses them to answer the input query
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Invoke the retrieval chain with the given input query to generate a response
        response = rag_chain.invoke({"input": input})

        # Extract the generated answer from the chain's response
        content = response['answer']
        
        return content

    def oci_vector_search_context_only(self, input: str, files_ids: list, k: int = 10) -> dict:
        """
        Performs a vector search and returns only the retrieved context without LLM processing.
        
        Args:
            input (str): The input query to search for relevant documents.
            files_ids (list): List of file IDs to filter the search.
            k (int): Number of documents to retrieve (default: 10).
            
        Returns:
            dict: Dictionary containing the retrieved context and metadata.
        """
        
        # Create a vector store from the Oracle connection and embeddings
        vector_store = self.rag_docs_db.get_vector_store()

        retriever = vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,           # Número de chunks relevantes que se devuelven
                "fetch_k": min(k * 2, 20),   # Número de candidatos iniciales desde los cuales aplicar MMR
                "filter": {"file_id": files_ids} # Limita la búsqueda al archivo específico
            }
        )

        # Retrieve documents without LLM processing
        retrieved_docs = retriever.get_relevant_documents(input)
        
        # Prepare context information
        context_info = {
            "query": input,
            "k_requested": k,
            "total_documents": len(retrieved_docs),
            "documents": []
        }
        
        for i, doc in enumerate(retrieved_docs):
            # Extract metadata and content from each document
            doc_info = {
                "index": i + 1,
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": getattr(doc, 'score', None),  # Score if available
                "file_id": doc.metadata.get('file_id', None),
                "chunk_id": doc.metadata.get('chunk_id', None)
            }
            context_info["documents"].append(doc_info)
        
        return context_info

    def oci_vector_search_raw_results(self, input: str, files_ids: list, k: int = 10) -> list:
        """
        Performs a vector search and returns raw search results with scores.
        
        Args:
            input (str): The input query to search for relevant documents.
            files_ids (list): List of file IDs to filter the search.
            k (int): Number of documents to retrieve (default: 10).
            
        Returns:
            list: List of raw search results with scores and metadata.
        """
        
        # Create a vector store from the Oracle connection and embeddings
        vector_store = self.rag_docs_db.get_vector_store()

        # Use similarity search with score to get raw results
        search_results = vector_store.similarity_search_with_score(
            input,
            k=k,
            filter={"file_id": files_ids}
        )
        
        # Format results
        formatted_results = []
        for i, (doc, score) in enumerate(search_results):
            result = {
                "rank": i + 1,
                "score": float(score),
                "content": doc.page_content,
                "metadata": doc.metadata,
                "file_id": doc.metadata.get('file_id', None),
                "chunk_id": doc.metadata.get('chunk_id', None)
            }
            formatted_results.append(result)
        
        return formatted_results